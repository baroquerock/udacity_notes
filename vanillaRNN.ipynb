{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass for Vanilla RNN\n",
    "Taken from here: https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('resources/shakespeare.txt', 'r').read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine own bright eyes,\n",
      "Feed'st thy light's flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thy self thy foe, to thy sweet self too cruel:\n",
      "Thou that art now the world's fresh ornament,\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content,\n",
      "And tender churl mak'st waste in niggarding:\n",
      "Pity the world, or else this glutton be,\n",
      "To eat the world's due, by the grave and thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[:610])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 9837 characters, 57 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "# char-to-int encoding\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print('data has {} characters, {} unique'.format(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Structure\n",
    "<img src=\"resources/rnn_schema.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components of RNN\n",
    "Taken from here: https://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf\n",
    "\n",
    "<img src=\"resources/rnn_components.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we defined the hyperparameters for the RNN layer as follows:<br>\n",
    "\n",
    "**hidden size** = 100<br>\n",
    "**sequence length** = 25\n",
    "\n",
    "\n",
    "So, what is the hidden size? \n",
    "\n",
    "In the example below we set the sequence length to 25. When we unroll the RNN layer, there will be 25 hidden states $h_{t}$. <br> Each hidden state will have the specified size of 100. So hidden size is just the size of every $h_{t}$. \n",
    "\n",
    "The output $h_{t}$ depends on two inputs:  \n",
    "(1) previous hidden state $h_{t-1}$  \n",
    "(2) the current element of the sequence $x_t$\n",
    "\n",
    "Regular neural networks use only one element $x$ as an input, <br>\n",
    "so there were only one matrix of parameters $W$. \n",
    "\n",
    "But in the case of RNN the output depends on TWO inputs, so we should have TWO parameter matrices instead of just one:\n",
    "\n",
    "$$h_{t} = tanh(W^{hh}*h_{t-1} + W^{hx}*x_{t} + bias)$$\n",
    "\n",
    "\n",
    "$W^{hx}$ is associated with the inputs.\n",
    "\n",
    "$W^{hh}$ is associated with the previous hidden state.<br><br>\n",
    "\n",
    "\n",
    "\n",
    "$h_{t}$ should have dimensions of $(hidden\\_size, 1)$, it is $(100, 1)$ in our case. <br>\n",
    "In order for this to happen, both terms $W^{hh}*h_{t-1}$ and $W^{hx}*x_{t}$ should have the same dimension of $(100, 1)$. <br>\n",
    "\n",
    "We already know that $h_{t-1}$ is $(100, 1)$, then $W^{hh}$ should have dimensions of $(100, 100)$, because:\n",
    "<br>\n",
    "$$(100, 100) * (100, 1) = (100, 1)$$\n",
    "<br>\n",
    "The $W^{hx}$ should have dimensions of $(100, vocab\\_size)$, because:\n",
    "<br>\n",
    "$$(100, vocab\\_size) * (vocab\\_size, 1) = (100, 1)$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to hidden weights: (100, 57)\n",
      "Hidden to hidden weights: (100, 100)\n",
      "Hidden to output weights: (57, 100)\n",
      "\n",
      "Hidden bias weights: (100, 1)\n",
      "Output bias weights: (57, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Input to hidden weights: {}'.format(Wxh.shape))\n",
    "print('Hidden to hidden weights: {}'.format(Whh.shape))\n",
    "print('Hidden to output weights: {}\\n'.format(Why.shape))\n",
    "print('Hidden bias weights: {}'.format(bh.shape))\n",
    "print('Output bias weights: {}'.format(by.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting one sequence\n",
    "For the purpose of demonstration, I am using only one sequence of length 25 in the forward pass. \n",
    "*inputs* - one sequence of length 25\n",
    "*targets* - one sequence of length 25 shifted by 1 over the *inputs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(45) -> r(41)\n",
      "r(41) -> o(14)\n",
      "o(14) -> m(24)\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "# word 'from'\n",
    "for i in range(3):\n",
    "    print('{}({}) -> {}({})'.format(ix_to_char[inputs[i]], inputs[i], ix_to_char[targets[i]], targets[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the previous hidden state\n",
    "# it equals to zeros since for the fisrt element there is no preious hidden state\n",
    "hprev = np.zeros((hidden_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs - contains one-hot encoded character in a sequence for timestep t\n",
    "# hs - contains hidden states for timestep t\n",
    "# ys - contains output for timestep t \n",
    "# ps - contains probabilities for timestep t \n",
    "xs, hs, ys, ps = {}, {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs[-1] = np.copy(hprev)\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(seq_length):\n",
    "    #print('Timestep: {}'.format(t+1))\n",
    "    xs[t] = np.zeros((vocab_size,1)) # one-hot\n",
    "    xs[t][inputs[t]] = 1 # one-hot    \n",
    "    first_term = np.dot(Wxh, xs[t])\n",
    "    second_term = np.dot(Whh, hs[t-1])\n",
    "    hs[t] = np.tanh(first_term + second_term + bh)\n",
    "    ys[t] = np.dot(Why, hs[t]) + by\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
